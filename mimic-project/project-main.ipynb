{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7304012",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DuckDB attached to remote PostgreSQL successfully.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9941f3e27dc342f6a9891c4a6ba893a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "RuntimeError",
     "evalue": "Query interrupted",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[31mKeyboardInterrupt\u001b[39m: ",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      3\u001b[39m attach_duckdb(\u001b[33m\"\u001b[39m\u001b[33mremote_mimic\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      4\u001b[39m query = load_sql(\u001b[33m\"\u001b[39m\u001b[33mrev-cohort.sql\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m df = \u001b[43mduckdb_to_df\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mInitial Data Shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdf.shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspaces/MHIA123-Project/mimic-project/src/db.py:39\u001b[39m, in \u001b[36mduckdb_to_df\u001b[39m\u001b[34m(query)\u001b[39m\n\u001b[32m     38\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mduckdb_to_df\u001b[39m(query):\n\u001b[32m---> \u001b[39m\u001b[32m39\u001b[39m     result = \u001b[43mcon\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m)\u001b[49m.df()\n\u001b[32m     40\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[31mRuntimeError\u001b[39m: Query interrupted"
     ]
    }
   ],
   "source": [
    "from src.db import attach_duckdb, duckdb_to_df, load_sql\n",
    "# 1. DATA ACQUISITION & PREPROCESSING\n",
    "attach_duckdb(\"remote_mimic\")\n",
    "df = duckdb_to_df(load_sql(\"rev-cohort.sql\"))\n",
    "print(f\"Initial Data Shape: {df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1695763",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "\n",
    "# --- Step A: Define Feature Groups ---\n",
    "# Continuous features need scaling + median imputation\n",
    "continuous_cols = ['anchor_age', 'heart_rate_mean', 'sbp_mean', 'dbp_mean', 'mbp_mean',\n",
    "                   'resp_rate_mean', 'spo2_mean', 'hematocrit', 'hemoglobin', 'wbc',\n",
    "                   'platelet', 'creatinine', 'bun', 'ck_mb']\n",
    "# Score/Binary features need 0 imputation\n",
    "score_cols = ['charlson_comorbidity_index', 'prev_mi', 'stroke_history']\n",
    "# --- Step B: Handle Missing Values (The \"Dual Strategy\") ---\n",
    "# 1. Impute Continuous with MEDIAN\n",
    "imputer = SimpleImputer(strategy='median')\n",
    "df[continuous_cols] = imputer.fit_transform(df[continuous_cols])\n",
    "# 2. Impute Scores with ZERO (Assume NULL = Absence of condition)\n",
    "df[score_cols] = df[score_cols].fillna(0)\n",
    "\n",
    "# --- Step C: Encoding ---\n",
    "le = LabelEncoder()\n",
    "df['gender'] = le.fit_transform(df['gender'])\n",
    "# Combine all features\n",
    "feature_cols = continuous_cols + score_cols + ['gender']\n",
    "X = df[feature_cols].values\n",
    "y = df['label'].values\n",
    "\n",
    "# --- Step D: Stratified Splitting ---\n",
    "# Stratify=y ensures we have the same % of mortality in Train, Val, and Test\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, stratify=y, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, stratify=y_temp, random_state=42)\n",
    "\n",
    "# --- Step E: Scaling (Standardization) ---\n",
    "# CRITICAL: Fit scaler ONLY on X_train to prevent info leakage from Test set\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_val = scaler.transform(X_val)\n",
    "X_test = scaler.transform(X_test)\n",
    "    \n",
    "# ==========================================\n",
    "# 3. Addressing Class Imbalance\n",
    "# ==========================================\n",
    "num_neg = (y_train == 0).sum()\n",
    "num_pos = (y_train == 1).sum()\n",
    "pos_weight_value = num_neg / num_pos\n",
    "\n",
    "print(\"-\" * 30)\n",
    "print(f\"Train Shape: {X_train.shape}\")\n",
    "print(f\"Class Balance (Train): {num_neg} Survivors vs {num_pos} Deaths\")\n",
    "print(f\"Calculated pos_weight: {pos_weight_value:.4f}\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "# ==========================================\n",
    "# 4. Prepare for PyTorch\n",
    "# ==========================================\n",
    "# Convert to Tensors\n",
    "X_train_tensor = torch.FloatTensor(X_train)\n",
    "y_train_tensor = torch.FloatTensor(y_train).unsqueeze(1)\n",
    "pos_weight_tensor = torch.FloatTensor([pos_weight_value])\n",
    "\n",
    "print(\"Ready for Model Training.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
