{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b7e85606",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching Cohort...\n",
      "Error attaching PostgreSQL: Binder Error: Failed to attach database: database with name \"mimic\" already exists\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e93646a23c84430796d8aa61b3ed6ba1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11726 patients in cohort fetched.\n"
     ]
    }
   ],
   "source": [
    "### FETCH COHORT FROM REMOTE DB ###\n",
    "\n",
    "import pandas as pd\n",
    "from src.db import attach_duckdb, load_sql, duckdb_to_df\n",
    "\n",
    "print(\"Fetching Cohort...\")\n",
    "attach_duckdb(\"mimic\")\n",
    "\n",
    "cohort_df = duckdb_to_df(load_sql(\"cohorts.sql\"))\n",
    "cohort_ids = tuple(cohort_df['hadm_id'].tolist())\n",
    "\n",
    "print(len(cohort_ids), \"patients in cohort fetched.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4a696082",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching Features...\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "### FETCH FEATURES ###\n",
    "\n",
    "print(\"Fetching Features...\")\n",
    "features_sql = load_sql(\"features.sql\")\n",
    "features_df = duckdb_to_df(features_sql.format(cohort_ids=cohort_ids))\n",
    "full_data = pd.merge(cohort_df, features_df, on='hadm_id', how='inner')\n",
    "\n",
    "print(\"Features fetched. Full data shape:\", full_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e781f951",
   "metadata": {},
   "outputs": [],
   "source": [
    "### DATA PREPROCESSING ###\n",
    "\n",
    "params = ['hr', 'map', 'crea', 'lac']\n",
    "\n",
    "for p in params:\n",
    "    full_data[f'{p}_base'] = full_data[f'{p}_base'].fillna(full_data[f'{p}_base'].median())\n",
    "    full_data[f'{p}_end'] = full_data[f'{p}_end'].fillna(full_data[f'{p}_end'].median())\n",
    "\n",
    "for p in params:\n",
    "    full_data[f'{p}_delta'] = full_data[f'{p}_end'] - full_data[f'{p}_base']\n",
    "\n",
    "feature_cols = [f'{p}_delta' for p in params] + [f'{p}_base' for p in params] + ['age', 'gender']\n",
    "full_data['gender'] = full_data['gender'].apply(lambda x: 1 if x == 'M' else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afb924ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "### SPLIT DATA TO TRAINING AND TEST SETS ###\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "X = full_data[feature_cols].values\n",
    "y = full_data['label'].values\n",
    "\n",
    "# 8. Split and Scale\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f90b161",
   "metadata": {},
   "outputs": [],
   "source": [
    "### MODEL DEFINITION ###\n",
    "\n",
    "from src.model import CardiacDataset, MortalityPredictor\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "train_dataset = CardiacDataset(X_train_scaled, y_train)\n",
    "test_dataset = CardiacDataset(X_test_scaled, y_test)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "model = MortalityPredictor(input_dim=X_train.shape[1])\n",
    "\n",
    "criterion = nn.BCELoss() # Binary Cross Entropy\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8802e547",
   "metadata": {},
   "outputs": [],
   "source": [
    "### MODEL TRAINING ###\n",
    "\n",
    "EPOCHS = 20\n",
    "print(\"Starting Training...\")\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        y_pred = model(X_batch)\n",
    "        loss = criterion(y_pred, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{EPOCHS} | Loss: {train_loss/len(train_loader):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23bdb5f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "### MODEL EVALUATION ###\n",
    "\n",
    "import torch\n",
    "from sklearn.metrics import roc_auc_score, classification_report\n",
    "import numpy as np\n",
    "\n",
    "model.eval()\n",
    "all_preds = []\n",
    "all_targets = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for X_batch, y_batch in test_loader:\n",
    "        y_pred = model(X_batch)\n",
    "        all_preds.extend(y_pred.numpy())\n",
    "        all_targets.extend(y_batch.numpy())\n",
    "\n",
    "y_pred_probs = np.array(all_preds)\n",
    "y_targets = np.array(all_targets)\n",
    "\n",
    "# Calculate AUC\n",
    "auc = roc_auc_score(y_targets, y_pred_probs)\n",
    "print(f\"\\nModel AUC-ROC: {auc:.4f}\")\n",
    "\n",
    "# Convert to binary predictions (Threshold 0.5)\n",
    "y_pred_binary = (y_pred_probs > 0.5).astype(int)\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_targets, y_pred_binary))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
