{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "51d8d00b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "sys.path.insert(0, str(Path.cwd().parent) if 'notebooks' in str(Path.cwd()) else str(Path.cwd()))\n",
    "\n",
    "from src.db import attach_duckdb, load_sql, duckdb_to_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b7e85606",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'get_db' from 'src.db' (/workspaces/MHIA123-Project/playground/src/db.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mos\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msrc\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdb\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m get_db, load_sql\n\u001b[32m      7\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mFetching Cohort...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      8\u001b[39m checkpoint_path = \u001b[33m\"\u001b[39m\u001b[33m/tmp/cohort_checkpoint.csv\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[31mImportError\u001b[39m: cannot import name 'get_db' from 'src.db' (/workspaces/MHIA123-Project/playground/src/db.py)"
     ]
    }
   ],
   "source": [
    "### FETCH COHORT FROM REMOTE DB (WITH CHECKPOINT) ###\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "from src.db import get_db, load_sql\n",
    "\n",
    "print(\"Fetching Cohort...\")\n",
    "checkpoint_path = \"/tmp/cohort_checkpoint.csv\"\n",
    "\n",
    "# Try to load from checkpoint if it exists\n",
    "if os.path.exists(checkpoint_path):\n",
    "    print(f\"Loading cohort from checkpoint ({checkpoint_path})...\")\n",
    "    cohort_df = pd.read_csv(checkpoint_path)\n",
    "else:\n",
    "    try:\n",
    "        db = get_db()\n",
    "        db.attach_postgres(dbname=\"mimic\")\n",
    "        cohort_df = db.execute_query(load_sql(\"cohorts.sql\"))\n",
    "        \n",
    "        # Save checkpoint for recovery\n",
    "        cohort_df.to_csv(checkpoint_path, index=False)\n",
    "        print(f\"✓ Saved cohort checkpoint to {checkpoint_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error loading cohort: {type(e).__name__}: {e}\")\n",
    "\n",
    "cohort_ids = tuple(cohort_df['hadm_id'].tolist())\n",
    "print(f\"✓ {len(cohort_ids)} patients in cohort\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4a696082",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching Features...\n",
      "Total cohort IDs: 11726\n",
      "Creating temporary table from cohort IDs...\n",
      "Executing optimized query...\n",
      "❌ Optimized query failed: BinderException: Binder Error: Catalog \"mimic\" does not exist!\n"
     ]
    }
   ],
   "source": [
    "### FETCH FEATURES (OPTIMIZED SQL WITH TEMP TABLE) ###\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from src.db import get_db, load_sql\n",
    "\n",
    "print(\"Fetching Features...\")\n",
    "print(f\"Total cohort IDs: {len(cohort_ids)}\")\n",
    "\n",
    "try:\n",
    "    db = get_db()\n",
    "    \n",
    "    # Create temp table from cohort_ids (much faster than IN clause)\n",
    "    print(\"Creating temporary table from cohort IDs...\")\n",
    "    cohort_ids_df = pd.DataFrame({'hadm_id': list(cohort_ids)})\n",
    "    db.create_temp_table(cohort_ids_df, table_name='temp_cohort')\n",
    "    \n",
    "    # Optimized SQL using temp table join instead of IN clause\n",
    "    optimized_query = \"\"\"\n",
    "    WITH cohort_filtered AS (\n",
    "        SELECT DISTINCT\n",
    "            adm.subject_id,\n",
    "            adm.hadm_id,\n",
    "            adm.admittime,\n",
    "            CASE \n",
    "                WHEN adm.hospital_expire_flag = 1 THEN adm.deathtime\n",
    "                ELSE adm.dischtime\n",
    "            END AS end_time\n",
    "        FROM mimic.mimiciv_hosp.admissions adm\n",
    "        INNER JOIN temp_cohort tc ON adm.hadm_id = tc.hadm_id\n",
    "    ),\n",
    "    vitals_agg AS (\n",
    "        SELECT \n",
    "            c.hadm_id,\n",
    "            AVG(CASE WHEN v.charttime <= c.admittime + INTERVAL '24' HOUR THEN v.heart_rate END) as hr_base,\n",
    "            AVG(CASE WHEN v.charttime <= c.admittime + INTERVAL '24' HOUR THEN v.mbp END) as map_base,\n",
    "            AVG(CASE WHEN v.charttime >= c.end_time - INTERVAL '24' HOUR THEN v.heart_rate END) as hr_end,\n",
    "            AVG(CASE WHEN v.charttime >= c.end_time - INTERVAL '24' HOUR THEN v.mbp END) as map_end\n",
    "        FROM cohort_filtered c\n",
    "        LEFT JOIN mimic.mimiciv_derived.vitalsign v \n",
    "            ON c.subject_id = v.subject_id\n",
    "            AND v.charttime BETWEEN c.admittime AND c.end_time\n",
    "        GROUP BY c.hadm_id\n",
    "    ),\n",
    "    labs_agg AS (\n",
    "        SELECT \n",
    "            c.hadm_id,\n",
    "            AVG(CASE WHEN l.charttime <= c.admittime + INTERVAL '24' HOUR THEN l.creatinine END) as crea_base,\n",
    "            AVG(CASE WHEN l.charttime <= c.admittime + INTERVAL '24' HOUR THEN l.lactate END) as lac_base,\n",
    "            AVG(CASE WHEN l.charttime >= c.end_time - INTERVAL '24' HOUR THEN l.creatinine END) as crea_end,\n",
    "            AVG(CASE WHEN l.charttime >= c.end_time - INTERVAL '24' HOUR THEN l.lactate END) as lac_end\n",
    "        FROM cohort_filtered c\n",
    "        LEFT JOIN mimic.mimiciv_derived.chemistry l\n",
    "            ON c.subject_id = l.subject_id\n",
    "            AND l.charttime BETWEEN c.admittime AND c.end_time\n",
    "        GROUP BY c.hadm_id\n",
    "    )\n",
    "    SELECT \n",
    "        c.hadm_id,\n",
    "        COALESCE(v.hr_base, 0) as hr_base,\n",
    "        COALESCE(v.map_base, 0) as map_base,\n",
    "        COALESCE(l.crea_base, 0) as crea_base,\n",
    "        COALESCE(l.lac_base, 0) as lac_base,\n",
    "        COALESCE(v.hr_end, 0) as hr_end,\n",
    "        COALESCE(v.map_end, 0) as map_end,\n",
    "        COALESCE(l.crea_end, 0) as crea_end,\n",
    "        COALESCE(l.lac_end, 0) as lac_end\n",
    "    FROM cohort_filtered c\n",
    "    LEFT JOIN vitals_agg v ON c.hadm_id = v.hadm_id\n",
    "    LEFT JOIN labs_agg l ON c.hadm_id = l.hadm_id\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"Executing optimized query...\")\n",
    "    features_df = db.execute_query(optimized_query)\n",
    "    db.unregister_temp_table('temp_cohort')\n",
    "    \n",
    "    print(f\"✓ Features DF fetched: {features_df.shape}\")\n",
    "    print(f\"✓ Features memory: {features_df.memory_usage(deep=True).sum() / 1e6:.2f} MB\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ Optimized query failed: {type(e).__name__}: {e}\")\n",
    "\n",
    "# Merge carefully\n",
    "#print(\"Merging cohort + features...\")\n",
    "#full_data = pd.merge(cohort_df, features_df, on='hadm_id', how='inner')\n",
    "#print(f\"✓ Merged data shape: {full_data.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fd8f9e0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== AFTER MERGE ===\n",
      "System: 4.33 GB avail | 47.3% used\n",
      "Process: 1765.51 MB\n",
      "Cohort DF: (11726, 9)\n",
      "Features DF: (1000, 9)\n",
      "Merged DF: (1000, 17)\n",
      "✓ Full data shape: (1000, 17)\n",
      "✓ Full data memory: 0.38 MB\n"
     ]
    }
   ],
   "source": [
    "### DATA VALIDATION & MEMORY CHECK ###\n",
    "import numpy as np\n",
    "\n",
    "def print_memory_status():\n",
    "    mem = psutil.virtual_memory()\n",
    "    pid = os.getpid()\n",
    "    proc = psutil.Process(pid)\n",
    "    print(f\"System: {mem.available/1e9:.2f} GB avail | {mem.percent}% used\")\n",
    "    print(f\"Process: {proc.memory_info().rss/1e6:.2f} MB\")\n",
    "\n",
    "print(\"=== AFTER MERGE ===\")\n",
    "print_memory_status()\n",
    "\n",
    "print(f\"Cohort DF: {cohort_df.shape}\")\n",
    "print(f\"Features DF: {features_df.shape}\")\n",
    "print(f\"Merged DF: {full_data.shape}\")\n",
    "\n",
    "# If dataset is still too large, downsample\n",
    "MAX_SAMPLES = 2000\n",
    "if len(full_data) > MAX_SAMPLES:\n",
    "    print(f\"⚠️ Dataset too large ({len(full_data)} > {MAX_SAMPLES}), downsampling...\")\n",
    "    full_data = full_data.sample(n=MAX_SAMPLES, random_state=42, stratify=full_data['label'] if 'label' in full_data.columns else None)\n",
    "    print(f\"✓ Downsampled to {len(full_data)} samples\")\n",
    "\n",
    "print(f\"✓ Full data shape: {full_data.shape}\")\n",
    "print(f\"✓ Full data memory: {full_data.memory_usage(deep=True).sum() / 1e6:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e781f951",
   "metadata": {},
   "outputs": [],
   "source": [
    "### DATA PREPROCESSING & VALIDATION ###\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Impute all numeric columns with median\n",
    "print(\"Imputing missing values...\")\n",
    "numeric_cols = full_data.select_dtypes(include=[np.number]).columns.tolist()\n",
    "for col in numeric_cols:\n",
    "    if col != 'label':  # Don't impute the target\n",
    "        full_data[col] = full_data[col].fillna(full_data[col].median())\n",
    "\n",
    "# Ensure gender is numeric\n",
    "if 'gender' in full_data.columns:\n",
    "    full_data['gender'] = full_data['gender'].apply(lambda x: 1 if str(x).upper() == 'M' else 0)\n",
    "\n",
    "# Define feature columns (include baseline and end values)\n",
    "feature_cols = [col for col in full_data.columns \n",
    "                if col not in ['hadm_id', 'subject_id', 'label', 'admittime', 'dischtime', 'deathtime', 'end_time']\n",
    "                and full_data[col].dtype in [np.float64, np.float32, np.int64, np.int32]]\n",
    "\n",
    "print(f\"Feature columns: {feature_cols}\")\n",
    "\n",
    "# ⚠️ CRITICAL: Validate data before model training\n",
    "print(\"Validating data integrity...\")\n",
    "assert len(full_data) > 0, \"❌ Data is empty after preprocessing!\"\n",
    "assert len(feature_cols) > 0, \"❌ No feature columns found!\"\n",
    "assert full_data[feature_cols].isna().sum().sum() == 0, f\"❌ NaN values in features: {full_data[feature_cols].isna().sum()}\"\n",
    "assert not np.isinf(full_data[feature_cols].values).any(), \"❌ Inf values in features!\"\n",
    "print(f\"✓ Data validated: {len(full_data)} samples, {len(feature_cols)} features\")\n",
    "if 'label' in full_data.columns:\n",
    "    print(f\"✓ Class balance: {(full_data['label'] == 1).sum()} positives, {(full_data['label'] == 0).sum()} negatives\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "afb924ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "### SPLIT DATA TO TRAINING AND TEST SETS ###\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "X = full_data[feature_cols].values\n",
    "y = full_data['label'].values\n",
    "\n",
    "# 8. Split and Scale\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1f90b161",
   "metadata": {},
   "outputs": [],
   "source": [
    "### MODEL DEFINITION ###\n",
    "\n",
    "from src.model import CardiacDataset, MortalityPredictor\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "train_dataset = CardiacDataset(X_train_scaled, y_train)\n",
    "test_dataset = CardiacDataset(X_test_scaled, y_test)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "model = MortalityPredictor(input_dim=X_train.shape[1])\n",
    "\n",
    "criterion = nn.BCELoss() # Binary Cross Entropy\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8802e547",
   "metadata": {},
   "outputs": [],
   "source": [
    "### MODEL TRAINING ###\n",
    "\n",
    "EPOCHS = 20\n",
    "print(\"Starting Training...\")\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        y_pred = model(X_batch)\n",
    "        loss = criterion(y_pred, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{EPOCHS} | Loss: {train_loss/len(train_loader):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "23bdb5f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "### MODEL EVALUATION ###\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score, classification_report, brier_score_loss\n",
    "\n",
    "model.eval()\n",
    "all_preds = []\n",
    "all_targets = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for X_batch, y_batch in test_loader:\n",
    "        X_batch = X_batch.to(device)\n",
    "        y_pred = model(X_batch)\n",
    "        all_preds.extend(y_pred.cpu().numpy())\n",
    "        all_targets.extend(y_batch.numpy())\n",
    "\n",
    "y_pred_probs = np.array(all_preds).flatten()\n",
    "y_targets = np.array(all_targets).flatten()\n",
    "\n",
    "# Calculate metrics\n",
    "auc = roc_auc_score(y_targets, y_pred_probs)\n",
    "brier = brier_score_loss(y_targets, y_pred_probs)\n",
    "\n",
    "print(f\"✓ Model Evaluation Results:\")\n",
    "print(f\"  AUC-ROC: {auc:.4f}\")\n",
    "print(f\"  Brier Score: {brier:.4f}\")\n",
    "\n",
    "# Binary predictions at 0.5 threshold\n",
    "y_pred_binary = (y_pred_probs > 0.5).astype(int)\n",
    "print(f\"\\nClassification Report:\")\n",
    "print(classification_report(y_targets, y_pred_binary))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
